{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG: Retrieval Augmented Generation.\n",
    "- Large language models (LLMs) have a limited context size.\n",
    "- TLDR\n",
    "- Not all context is relevant to a given question\n",
    "- Query -> Search -> Results -> (LLM) -> Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -qU  markdownify  langchain-upstage rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# UPSTAGE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "\n",
    "\n",
    "layzer = UpstageLayoutAnalysisLoader(\"pdfs/kim-tse-2008.pdf\", output_type=\"html\")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "docs = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br><p id='2' style='font-size:22px'>Classifying Software Changes:<br>Clean or Buggy?</p><br><p id='3' style='font-size:20px'>Sunghun Kim, E. James Whitehead Jr., Member , IEEE , and Yi Zhang, Member , IEEE</p><p id='4' style='font-size:16px'>Abstract —This paper introduces a new technique for predicting latent software bugs, called change classification. Change<br>classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes or<br>clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using<br>features (in the machine learning sense) extracted from the revision history of a software project stored in its software configuration<br>management repository. The trained classifier can classify changes as buggy or clean, with a 78 percent accuracy and a 60 percent<br>buggy change recall on average. Change classification has several desirable qualit"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(docs[0].page_content[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide most correct answer from the following context. \n",
    "    If the answer is not present in the context, please write \"The information is not present in the context.\"\n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {Context}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sure, here is the code to extract the text from the PDF files and store it in a PostgreSQL database:\\n```python\\nimport os\\nimport pdfplumber\\nimport psycopg2\\n\\n# Set up connection to PostgreSQL database\\nconn = psycopg2.connect(\\n    host=\"your_host\",\\n    database=\"your_database\",\\n    user=\"your_user\",\\n    password=\"your_password\"\\n)\\n\\n# Create a cursor object to execute SQL queries\\ncur = conn.cursor()\\n\\n# Define the table to store the extracted text\\ncur.execute(\"\"\"\\n    CREATE TABLE IF NOT EXISTS pdf_text (\\n        file_name TEXT,\\n        text_content TEXT\\n    )\\n\"\"\")\\n\\n# Define the directory containing the PDF files\\npdf_dir = \"/path/to/pdf/directory\"\\n\\n# Loop through each PDF file in the directory\\nfor file_name in os.listdir(pdf_dir):\\n    if file_name.endswith(\".pdf\"):\\n        # Open the PDF file using pdfplumber\\n        with pdfplumber.open(os.path.join(pdf_dir, file_name)) as pdf:\\n            # Extract all the text in the PDF file\\n            text_content = \"\"\\n            for page in pdf.pages:\\n                text_content += page.extract_text()\\n\\n            # Insert the extracted text into the database\\n            cur.execute(\"\"\"\\n                INSERT INTO pdf_text (file_name, text_content)\\n                VALUES (%s, %s)\\n            \"\"\", (file_name, text_content))\\n\\n# Commit the changes to the database\\nconn.commit()\\n\\n# Close the cursor and connection\\ncur.close()\\nconn.close()\\n```\\nNote that you will need to replace `your_host`, `your_database`, `your_user`, and `your_password` with the appropriate values for your PostgreSQL database. You will also need to replace `/path/to/pdf/directory` with the actual path to the directory containing your PDF files.\\n\\nThis code will extract all the text from each PDF file in the specified directory and store it in a PostgreSQL database table called `pdf_text`. The table has two columns: `file_name` and `text_content`. The `file_name` column stores the name of the PDF file, and the `text_content` column stores the extracted text.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is bug classficiation?\", \"Context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "retriever = BM25Retriever.from_documents(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"two changes were made. The<br>function bar was renamed to foo and println has<br>argument “ report.str ” instead of “ report. ” As a result,<br>the annotate output shows lines 1 and 4 as having<br>been most recently modified in revision 2 by “ ejw .”<br>. Revision 3 shows a change, the actual bug fix,<br>changing line 3 from “==” to “ != .”</p><br><p id='98' style='font-size:18px'>The SZZ algorithm then identifies the bug-introducing<br>change associated with the bug fix in revision 3. It starts by<br>computing the delta between revisions 3 and 2, yielding</p><p id='101' style='font-size:16px'>line 3. SZZ then uses the SCM annotate data to determine<br>the initial origin of line 3 at revision 2. This is revision 1, the<br>bug-introducing change.</p><br><p id='102' style='font-size:16px'>One assumption of the presentation so far is that a bug is<br>repaired in a single bug-fix change. What happens when a<br>bug is repaired across multiple commits? There are two<br>cases. In the first\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none'}),\n",
       " Document(page_content='happens when a<br>bug is repaired across multiple commits? There are two<br>cases. In the first case, a bug repair is split across multiple<br>commits, with each commit modifying a separate section of<br>the code (code sections are disjoint). Each separate change is<br>tracked back to its initial bug-introducing change, which is<br>then used to train the SVM classifier. In the second case, a bug<br>fix occurs incrementally over multiple commits, with some<br>later fixes modifying earlier ones (the fix code partially<br>overlaps). The first patch in an overlapping code section<br>would be traced back to the original bug-introducing change.<br>Later modifications would not be traced back to the original<br>bug-introducing change. Instead, they would be traced back<br>to an intermediate modification, which is identified as bug<br>introducing. This is appropriate since the intermediate<br>modification did not correctly fix the bug and, hence, is<br>simultaneously a bug fix and buggy. In', metadata={'total_pages': 16, 'type': 'html', 'split': 'none'}),\n",
       " Document(page_content=\"If a keyword or bug report reference<br>is found, the changes in the associated commit comprise a<br>bug fix. Table 3 lists keywords or phrases used to identify<br>bug-fix commits. Manual verification of identified bug-fix<br>changes is recommended to ensure that the selected<br>keywords or phrases are correctly identifying bug-fix<br>changes. For example, if a commit log stated, “This is not<br>a bug fix,” its commit should not be identified as a fix. For<br>the systems studied in this paper, one of the authors<br>manually verified that the identified fix commits were<br>indeed fixes.</p><br><p id='91' style='font-size:18px'>One potential issue of identifying bug fixes using the bug<br>tracking system identifiers is the common use of bug<br>tracking systems to record both bug reports and new<br>feature additions. This causes new feature changes to be<br>identified as bug-fix changes. Among the systems studied<br>in this paper, Bugzilla and Scarab both use bug tracking<br>systems to\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none'}),\n",
       " Document(page_content=\"ECHNIQUES</p><br><p id='124' style='font-size:18px'>Among many classification algorithms, Support Vector<br>Machine (SVM) [14] is used to implement and evaluate<br>the change classification approach for bug prediction<br>because it is a high-performance algorithm that is com-<br>monly used across a wide range of text classification<br>applications. Several good quality implementations of<br>SVM are readily available. The Weka Toolkit [52] imple-<br>mentation is used in this study. In the following, we<br>provide an overview description of SVM and then describe<br>the measures used in our evaluation of SVM for change<br>classification. There is substantial literature on SVM. The<br>interested reader is encouraged to pursue [14] or [49] for an<br>in-depth description.</p><br><p id='125' style='font-size:20px'>5.1 Overview of Support Vector Machines</p><br><p id='126' style='font-size:18px'>SVMs were originally designed for binary classification,<br>where the class label can take only\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none'})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is bug classficiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The information is not present in the context.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is bug classficiation?\"\n",
    "context_docs = retriever.invoke(query)\n",
    "chain.invoke({\"question\": query, \"Context\": context_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bug classification refers to the process of predicting whether there is a bug in any of the lines that were changed in one file in one SCM commit transaction. It differs from previous bug prediction work that focuses on finding prediction or regression models to identify fault-prone or buggy modules, files, and functions. Instead, bug classification predicts the presence of bugs in specific code changes. It uses bug-introducing changes, which contain the exact commit/line changes that injected a bug, to label changes as buggy or clean. Additionally, it utilizes features from the source code, such as variable names, method calls, operators, constants, and comment text, to train the classification models.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is bug classficiation?\"\n",
    "context_docs = retriever.invoke(\"bug\")\n",
    "chain.invoke({\"question\": query, \"Context\": context_docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise \n",
    "It seems keyword search is not the best for LLM queries. What are some alternatives?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

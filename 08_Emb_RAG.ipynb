{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG: Retrieval Augmented Generation.\n",
    "- Large language models (LLMs) have a limited context size.\n",
    "- TLDR\n",
    "- Not all context is relevant to a given question\n",
    "- Query -> Search -> Results -> (LLM) -> Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword VS Semantic Search \n",
    "![Vector](https://blog.dataiku.com/hs-fs/hubfs/dftt%202.webp?width=1346&height=632&name=dftt%202.webp)\n",
    "\n",
    "from https://blog.dataiku.com/semantic-search-an-overlooked-nlp-superpower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Emb_search](figures/emb_search.png)\n",
    "\n",
    "from https://sreent.medium.com/llms-embeddings-and-vector-search-d4bd9362df56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -qU  markdownify  langchain-upstage rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext dotenv\n",
    "%dotenv\n",
    "# UPSTAGE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "embeddings_model = UpstageEmbeddings()\n",
    "embeddings = embeddings_model.embed_documents(\n",
    "    [\n",
    "        \"Whay is the best season to visit Korea?\",\n",
    "    ])\n",
    "\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG 1. load doc (done), 2. chunking, splits, 3. embeding - indexing, 4. retrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "\n",
    "\n",
    "layzer = UpstageLayoutAnalysisLoader(\"kim-tse-2008.pdf\", output_type=\"html\")\n",
    "# For improved memory efficiency, consider using the lazy_load method to load documents page by page.\n",
    "docs = layzer.load()  # or layzer.lazy_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "print(\"Splits:\", len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "# 3. Embed & indexing\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=UpstageEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_upstage import ChatUpstage\n",
    "\n",
    "\n",
    "llm = ChatUpstage()\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Please provide most correct answer from the following context. \n",
    "    If the answer is not present in the context, please write \"The information is not present in the context.\"\n",
    "    ---\n",
    "    Question: {question}\n",
    "    ---\n",
    "    Context: {Context}\n",
    "    \"\"\"\n",
    ")\n",
    "chain = prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bug classification is a technique used in software development to predict the presence of latent software bugs in file-level changes by using machine learning classification algorithms. The technique involves two steps: training and classification. The change classification algorithms learn from a training set, which is a collection of changes that are known to belong to an existing class (buggy or clean changes). Features are extracted from the changes, and the classification algorithm learns which features are the most useful for discriminating among the various classes. The trained classifier can then classify new changes as buggy or clean. The goal of bug classification is to use a machine learning classifier to predict bugs in changes. The change classification technique is programming language independent since it uses a bag-of-words method for generating features from the source code. The projects that were analyzed span many popular current programming languages, including C/C++, Java, Perl, Python, Java Script, PHP, and XML.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is bug classficiation?\", \"Context\": docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "retriever = BM25Retriever.from_documents(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"two changes were made. The<br>function bar was renamed to foo and println has<br>argument “ report.str ” instead of “ report. ” As a result,<br>the annotate output shows lines 1 and 4 as having<br>been most recently modified in revision 2 by “ ejw .”<br>. Revision 3 shows a change, the actual bug fix,<br>changing line 3 from “==” to “ != .”</p><br><p id='98' style='font-size:18px'>The SZZ algorithm then identifies the bug-introducing<br>change associated with the bug fix in revision 3. It starts by<br>computing the delta between revisions 3 and 2, yielding</p><p id='101' style='font-size:16px'>line 3. SZZ then uses the SCM annotate data to determine<br>the initial origin of line 3 at revision 2. This is revision 1, the<br>bug-introducing change.</p><br><p id='102' style='font-size:16px'>One assumption of the presentation so far is that a bug is<br>repaired in a single bug-fix change. What happens when a<br>bug is repaired across multiple commits? There are two<br>cases. In the first\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none'}),\n",
       " Document(page_content='happens when a<br>bug is repaired across multiple commits? There are two<br>cases. In the first case, a bug repair is split across multiple<br>commits, with each commit modifying a separate section of<br>the code (code sections are disjoint). Each separate change is<br>tracked back to its initial bug-introducing change, which is<br>then used to train the SVM classifier. In the second case, a bug<br>fix occurs incrementally over multiple commits, with some<br>later fixes modifying earlier ones (the fix code partially<br>overlaps). The first patch in an overlapping code section<br>would be traced back to the original bug-introducing change.<br>Later modifications would not be traced back to the original<br>bug-introducing change. Instead, they would be traced back<br>to an intermediate modification, which is identified as bug<br>introducing. This is appropriate since the intermediate<br>modification did not correctly fix the bug and, hence, is<br>simultaneously a bug fix and buggy. In', metadata={'total_pages': 16, 'type': 'html', 'split': 'none'}),\n",
       " Document(page_content=\"If a keyword or bug report reference<br>is found, the changes in the associated commit comprise a<br>bug fix. Table 3 lists keywords or phrases used to identify<br>bug-fix commits. Manual verification of identified bug-fix<br>changes is recommended to ensure that the selected<br>keywords or phrases are correctly identifying bug-fix<br>changes. For example, if a commit log stated, “This is not<br>a bug fix,” its commit should not be identified as a fix. For<br>the systems studied in this paper, one of the authors<br>manually verified that the identified fix commits were<br>indeed fixes.</p><br><p id='91' style='font-size:18px'>One potential issue of identifying bug fixes using the bug<br>tracking system identifiers is the common use of bug<br>tracking systems to record both bug reports and new<br>feature additions. This causes new feature changes to be<br>identified as bug-fix changes. Among the systems studied<br>in this paper, Bugzilla and Scarab both use bug tracking<br>systems to\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none'}),\n",
       " Document(page_content=\"ECHNIQUES</p><br><p id='124' style='font-size:18px'>Among many classification algorithms, Support Vector<br>Machine (SVM) [14] is used to implement and evaluate<br>the change classification approach for bug prediction<br>because it is a high-performance algorithm that is com-<br>monly used across a wide range of text classification<br>applications. Several good quality implementations of<br>SVM are readily available. The Weka Toolkit [52] imple-<br>mentation is used in this study. In the following, we<br>provide an overview description of SVM and then describe<br>the measures used in our evaluation of SVM for change<br>classification. There is substantial literature on SVM. The<br>interested reader is encouraged to pursue [14] or [49] for an<br>in-depth description.</p><br><p id='125' style='font-size:20px'>5.1 Overview of Support Vector Machines</p><br><p id='126' style='font-size:18px'>SVMs were originally designed for binary classification,<br>where the class label can take only\", metadata={'total_pages': 16, 'type': 'html', 'split': 'none'})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is bug classficiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The information is not present in the context.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is bug classficiation?\"\n",
    "context_docs = retriever.invoke(query)\n",
    "chain.invoke({\"question\": query, \"Context\": context_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Change classification is a process that classifies changes in source code. It is different from previous bug prediction work because it focuses on predicting whether there is a bug in any of the lines that were changed in one file in one SCM commit transaction, rather than making bug predictions at the module, file, or method level. Change classification uses bug-introducing changes, which contain the exact commit/line changes that introduced a bug, and uses features from the source code, such as variables, method calls, operators, constants, and comment text, to train the change classification models.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is bug classficiation?\"\n",
    "context_docs = retriever.invoke(\"bug\")\n",
    "chain.invoke({\"question\": query, \"Context\": context_docs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise \n",
    "It seems keyword search is not the best for LLM queries. What are some alternatives?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
